{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLabCut, getting ready\n",
    "\n",
    "\n",
    "In this project, we will use DeepLabCut to detect fingers. We will assume that we don't have a computer with a powerful GPU. We will use GPUs provided by Google to train and evaluate our network.\n",
    "\n",
    "\n",
    "## Install Anaconda\n",
    "\n",
    "The first step is to install python and all the python packages needed. The easiest way is to download and install Anaconda.\n",
    "\n",
    "https://www.anaconda.com/\n",
    "\n",
    "\n",
    "## Get DeepLabCut from GitHub\n",
    "\n",
    "https://github.com/AlexEMG/DeepLabCut\n",
    "\n",
    "Run the following command to clone the repository to you computer. This is not python code but should be run in a terminal. \n",
    "\n",
    "```git clone https://github.com/AlexEMG/DeepLabCut.git```\n",
    "\n",
    "There are several tutorials in the DeepLabCut repository. They can be very useful.\n",
    "\n",
    "\n",
    "## Install the DeepLabCut Anaconda environment\n",
    "\n",
    "The installation will depend on whether you want to use a GPU on your computer to train the network or not. Here I will assume that you don't have a poweful GPU available in your computer. We will create the project and label some frames on our computer, then move our project to google drives, train the network with Colab GPUs.\n",
    "\n",
    "The latest instructions to install dlc can be found here.\n",
    "\n",
    "https://github.com/AlexEMG/DeepLabCut/blob/master/docs/installation.md\n",
    "\n",
    "\n",
    "```\n",
    "cd DeepLabCut/conda-environments\n",
    "conda env create -f dlc-ubuntu-GPU.yaml\n",
    "```\n",
    "\n",
    "```\n",
    "conda activate dlc-ubuntu-GPU\n",
    "```\n",
    "\n",
    "## Install jupyter notebook\n",
    "\n",
    "I did not have jupyter notebook installed in my dlc-ubuntu-GPU environment, so I installed it. \n",
    "\n",
    "```\n",
    "conda install -c anaconda jupyter\n",
    "```\n",
    "\n",
    "## Install tensorflow-cpu\n",
    "\n",
    "The dlc-ubuntu-GPU environment comes with tensorflow-gpu. Since I don't have a gpu on my laptop, I change this to a cpu version.\n",
    "\n",
    "```\n",
    "pip install --ignore-installed --upgrade tensorflow==1.9\n",
    "```\n",
    "\n",
    "We are now done preparing our working environment.\n",
    "\n",
    "## Make sure your dlc environment is activated\n",
    "\n",
    "Make sure your DeepLabCut anaconda environment is activated in the terminal you used to run your jupyter notebook.\n",
    "Otherwise, DeepLabCut will not be available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with DeepLabCut\n",
    "\n",
    "Now we can get going with actually working with deeplabcut.\n",
    "\n",
    "The first thing to do is to load the deeplabcut package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/anaconda3/envs/dlc-ubuntu-GPU/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kevin/anaconda3/envs/dlc-ubuntu-GPU/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kevin/anaconda3/envs/dlc-ubuntu-GPU/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kevin/anaconda3/envs/dlc-ubuntu-GPU/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kevin/anaconda3/envs/dlc-ubuntu-GPU/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kevin/anaconda3/envs/dlc-ubuntu-GPU/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import deeplabcut "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new project\n",
    "\n",
    "It is always good idea to keep the projects seperate if you want to use different networks to analze your data. You should use one project if you are tracking similar subjects/items even if in different environments. This function creates a new project with sub-directories and a basic configuration file in the user defined directory otherwise the project is created in the current working directory.\n",
    "\n",
    "You can always add new videos (for lableing more data) to the project at any stage of the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created \"/home/kevin/repo/dataNeuroCourse/DeepLabCut/digit_tracking-Allen-2019-12-18/videos\"\n",
      "Created \"/home/kevin/repo/dataNeuroCourse/DeepLabCut/digit_tracking-Allen-2019-12-18/labeled-data\"\n",
      "Created \"/home/kevin/repo/dataNeuroCourse/DeepLabCut/digit_tracking-Allen-2019-12-18/training-datasets\"\n",
      "Created \"/home/kevin/repo/dataNeuroCourse/DeepLabCut/digit_tracking-Allen-2019-12-18/dlc-models\"\n",
      "Copying the videos\n",
      "/home/kevin/repo/dataNeuroCourse/DeepLabCut/digit_tracking-Allen-2019-12-18/videos/digits.avi\n",
      "Generated \"/home/kevin/repo/dataNeuroCourse/DeepLabCut/digit_tracking-Allen-2019-12-18/config.yaml\"\n",
      "\n",
      "A new project with name digit_tracking-Allen-2019-12-18 is created at /home/kevin/repo/dataNeuroCourse/DeepLabCut and a configurable file (config.yaml) is stored there. Change the parameters in this file to adapt to your project's needs.\n",
      " Once you have changed the configuration file, use the function 'extract_frames' to select frames for labeling.\n",
      ". [OPTIONAL] Use the function 'add_new_videos' to add new videos to your project (at any stage).\n",
      "/home/kevin/repo/dataNeuroCourse/DeepLabCut/digit_tracking-Allen-2019-12-18/config.yaml\n"
     ]
    }
   ],
   "source": [
    "task='digit_tracking' # Enter the name of your experiment Task\n",
    "experimenter='Allen' # Enter the name of the experimenter\n",
    "video=['/home/kevin/repo/dataNeuroCourse/DeepLabCut/video/digits.avi'] # Enter the paths of your videos OR FOLDER you want to grab frames from.\n",
    "\n",
    "path_config_file=deeplabcut.create_new_project(task,experimenter,video,copy_videos=True) \n",
    "\n",
    "print(path_config_file)\n",
    "# NOTE: The function returns the path, where your project is. \n",
    "# You could also enter this manually \n",
    "# (e.g. if the project is already created and you want to pick up, where you stopped...)\n",
    "#path_config_file = '/home/Mackenzie/Reaching/config.yaml' \n",
    "# Enter the path of the config file that was just created from the above step (check the folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit the config.yaml file that was created! \n",
    "Add your body part labels, edit the number of frames to extract per video, etc. \n",
    "\n",
    "I will use my favorite text editor emacs\n",
    "\n",
    "```\n",
    "emacs /home/kevin/repo/dataNeuroCourse/DeepLabCut/digit_tracking-Allen-2019-12-18/config.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract frames from our video\n",
    "\n",
    "A key point for a successful feature detector is to select diverse frames, which are typical for the behavior you study that should be labeled.\n",
    "\n",
    "This function selects N frames either uniformly sampled from a particular video (or folder) ('uniform'). Note: this might not yield diverse frames, if the behavior is sparsely distributed (consider using kmeans), and/or select frames manually etc.\n",
    "\n",
    "Also make sure to get select data from different (behavioral) sessions and different animals if those vary substantially (to train an invariant feature detector).\n",
    "\n",
    "Individual images should not be too big (i.e. < 850 x 850 pixel). Although this can be taken care of later as well, it is advisable to crop the frames, to remove unnecessary parts of the frame as much as possible.\n",
    "\n",
    "Always check the output of cropping. If you are happy with the results proceed to labeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file read successfully.\n",
      "Do you want to extract (perhaps additional) frames for video: /home/kevin/repo/dataNeuroCourse/DeepLabCut/digit_tracking-Allen-2019-12-18/videos/digits.avi ?\n",
      "yes/noyes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [00:00, 705.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 12.98  seconds.\n",
      "Extracting and downsampling... 389  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "389it [00:00, 783.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "\n",
      "Frames were selected.\n",
      "You can now label the frames using the function 'label_frames' (if you extracted enough frames for all videos).\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#there are other ways to grab frames, such as uniformly; please see the paper:\n",
    "\n",
    "#AUTOMATIC:\n",
    "deeplabcut.extract_frames(path_config_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label the extracted frames\n",
    "\n",
    "Only videos in the config file can be used to extract the frames. Extracted labels for each video are stored in the project directory under the subdirectory **'labeled-data'**. Each subdirectory is named after the name of the video. The toolbox has a labeling toolbox which could be used for labeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can now check the labels, using 'check_labels' before proceeding. Then, you can use the function 'create_training_dataset' to create the training dataset.\n"
     ]
    }
   ],
   "source": [
    "%gui wx\n",
    "deeplabcut.label_frames(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move your project directory to google drives and use the colab notebook to train the network\n",
    "\n",
    "We will move our project to google drive and use [colab](https://colab.research.google.com/notebooks/welcome.ipynb) to train our network.\n",
    "\n",
    "Copy the entire project directory to your google drive.\n",
    "\n",
    "The code to train our deep neural network is in the `DeepLabCut` directory of the course repository (`dlc_colab_digit.ipynb`). Copy it to your google drive and open it with colab.\n",
    "\n",
    "You will need to edit the path of your project in the config.yaml file of the project so that it points to your porject directory on the google drive. In my case I used:\n",
    "\n",
    "```\n",
    "project_path: /content/drive/My Drive/teaching_and_thesis/master_neuroscience_2019/deeplabcut/digit_tracking-Allen-2019-12-18\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
